{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365914e-cf32-4b48-824b-ea69b8f0cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkc_k\\Downloads\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "import gradio as gr\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "import datasketch  # MinHash & LSH\n",
    "import re\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b7cee7-417f-46a3-8b33-6538455e03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data using pandas' read_json file.\n",
    "data = pd.read_json('tops_fashion.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5235651-3d69-491f-816f-26c4b4028261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66247dc8-90f8-456f-b41d-5f073609bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating price=NULL : 28395\n"
     ]
    }
   ],
   "source": [
    "# consider products which have price information\n",
    "# data['formatted_price'].isnull() => gives the information \n",
    "#about the dataframe row's which have null values price == None|Null\n",
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f2103b-ef87-491b-8587-297fcf424f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating color=NULL : 28385\n"
     ]
    }
   ],
   "source": [
    "# consider products which have color information\n",
    "# data['color'].isnull() => gives the information about the dataframe row's which have null values price == None|Null\n",
    "data =data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating color=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c775947a-1e04-4417-b787-2549b5792056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of products with short description: 27949\n"
     ]
    }
   ],
   "source": [
    "# Remove All products with very few words in title\n",
    "data_sorted = data[data['title'].apply(lambda x: len(x.split())>4)]\n",
    "print(\"After removal of products with short description:\", data_sorted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd90f73-4009-4078-a265-d711c5a4c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the whole data based on title (alphabetical order of title) \n",
    "data_sorted.sort_values('title',inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0fe217-6a3a-400c-bc26-f542f712240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ff9d5-7bb9-4eda-96d6-ed6b1f96c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stage1_dedupe_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    \n",
    "    previous_i = i\n",
    "\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "\n",
    "    # search for the similar products sequentially \n",
    "    j = i+1\n",
    "    while j < num_data_points:\n",
    "\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'Small']\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "\n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a), len(b))\n",
    "\n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different\n",
    "        # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) > 2: # number of words in which both sensences differ\n",
    "            # if both strings are differ by more than 2 words we include the 1st string index\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "\n",
    "            # if the comaprision between is between num_data_points, num_data_points-1 strings and they differ in more than 2 words we include both\n",
    "            if j == num_data_points-1: stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "\n",
    "            # start searching for similar apperals corresponds 2nd string\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60c1ce8-55b2-4367-ba03-30ba780c214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f019d3c-2c6f-4d50-83ba-a887d7765207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ASINs count: 16706\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Build MinHash Signatures\n",
    "lsh = datasketch.MinHashLSH(threshold=0.8, num_perm=128)  # 80% similarity threshold\n",
    "minhashes = {}\n",
    "asin_dict = {}\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    if pd.isna(row['title']):  # Handle missing titles\n",
    "        continue\n",
    "\n",
    "    words = set(re.findall(r\"\\w+\", row['title'].lower()))  # Better tokenization\n",
    "    minhash = datasketch.MinHash(num_perm=128)  # Create MinHash signature\n",
    "\n",
    "    for word in words:\n",
    "        minhash.update(word.encode('utf8'))  # Hash words into signature\n",
    "    \n",
    "    lsh.insert(i, minhash)  # Insert into LSH index\n",
    "    minhashes[i] = minhash\n",
    "    asin_dict[i] = data.iloc[i]['asin']\n",
    "\n",
    "# Step 2: Find Similar Titles using LSH\n",
    "considered_asins = set()\n",
    "stage2_dedupe_asins = []\n",
    "\n",
    "for i in minhashes.keys():  # Only loop over existing MinHashes\n",
    "    if i in considered_asins:\n",
    "        continue\n",
    "\n",
    "    stage2_dedupe_asins.append(asin_dict[i])\n",
    "    considered_asins.add(i)\n",
    "\n",
    "    # Find similar items using LSH (FAST lookup)\n",
    "    similar_items = lsh.query(minhashes[i])\n",
    "\n",
    "    for j in similar_items:\n",
    "        if j != i and j not in considered_asins:\n",
    "            considered_asins.add(j)  # Mark as duplicate\n",
    "\n",
    "# Output deduplicated ASINs\n",
    "print(\"Unique ASINs count:\", len(stage2_dedupe_asins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c1c138-d522-4883-9aad-c06017e76aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from whole previous products we will consider only \n",
    "# the products that are found in previous cell \n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7afa7d8-6f03-4557-8f8c-33471d2d0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of stop words: {\"shan't\", 'be', 'so', 'through', 'here', 'what', \"it's\", 'that', \"i'm\", 'ourselves', 'has', \"we've\", \"you're\", \"weren't\", 'at', 'and', 'does', 're', 'those', 'his', 'yours', 'needn', 'wasn', 'her', 'your', 'i', \"didn't\", 'ain', 'should', \"i've\", 'we', 'is', \"mustn't\", 'ours', 'are', \"they'd\", \"wasn't\", 'shan', 'all', 'herself', 'aren', 'not', 'then', 'won', 'further', 'haven', 'most', 'about', 'off', 'on', 'why', 'yourselves', \"hadn't\", 'between', 'were', \"we're\", 's', 'do', 'will', \"they'll\", \"don't\", 'after', 'couldn', 've', \"mightn't\", 'an', \"she's\", 'for', 'only', \"aren't\", 'once', 'their', 'mightn', 't', 'both', \"haven't\", 'from', 'a', 'having', 'as', \"they're\", \"should've\", 'but', 'me', 'being', \"he's\", 'to', 'been', 'in', 'ma', 'until', \"you'll\", 'did', 'hers', 'than', \"they've\", \"doesn't\", 'hasn', 'who', \"couldn't\", 'them', 'above', \"you'd\", \"we'd\", \"i'd\", 'before', 'the', 'doing', \"that'll\", 'because', 'it', 'if', \"she'd\", 'how', 'where', \"shouldn't\", 'she', 'under', \"won't\", 'some', 'over', 'isn', 'doesn', 'by', 'whom', 'its', 'into', 'themselves', 'am', 'itself', 'when', 'yourself', \"he'd\", \"needn't\", 'll', \"i'll\", 'this', 'o', 'had', 'same', 'mustn', \"it'd\", 'don', 'him', 'against', 'you', \"he'll\", 'can', 'they', 'down', 'during', 'such', \"we'll\", 'no', 'up', 'more', 'wouldn', 'our', 'just', \"you've\", 'with', \"hasn't\", 'there', 'which', 'shouldn', 'now', 'too', \"wouldn't\", 'below', 'out', 'other', \"she'll\", 'was', 'he', 'my', 'few', 'm', 'very', 'or', 'himself', 'these', 'myself', \"isn't\", 'any', 'didn', 'hadn', 'while', 'of', 'y', 'nor', 'theirs', 'd', 'again', 'have', 'weren', 'own', 'each', \"it'll\"}\n"
     ]
    }
   ],
   "source": [
    "# we use the list of stop words that are downloaded from nltk lib.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print ('list of stop words:', stop_words)\n",
    "\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            # Conver all letters to lower-case\n",
    "            word = word.lower()\n",
    "            # stop-word removal\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        data[column][index] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a703a1a-780c-44b8-bf38-785ccbf8b5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.4375 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "# we take each title and we text-preprocess it.\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing(row['title'], index, 'title')\n",
    "# we print the time it took to preprocess whole titles \n",
    "print(time.process_time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533adfae-9b4b-497a-9bd0-7b360afcf274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f89491c5-efc8-412a-b357-665f4faad80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████| 523/523 [05:42<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# transformer \n",
    "# Load a strong pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Titles of products in your dataset\n",
    "titles = data['title'].tolist()\n",
    "\n",
    "# image urls\n",
    "image_urls = data['medium_image_url'].tolist()\n",
    "\n",
    "# Generate SBERT embeddings\n",
    "sbert_embeddings = sbert_model.encode(titles, show_progress_bar=True)\n",
    "\n",
    "def sbert_recommender_gradio(input_title, num_results=5):\n",
    "    # Encode input title\n",
    "    query_vec = sbert_model.encode([input_title])\n",
    "    \n",
    "    # Compute cosine similarity with dataset\n",
    "    sim_scores = cosine_similarity(query_vec, sbert_embeddings)[0]\n",
    "    \n",
    "    # Add similarity scores to the DataFrame\n",
    "    df = data.copy()\n",
    "    df['score'] = sim_scores\n",
    "\n",
    "    # Get top N matches\n",
    "    top_matches = df.sort_values(by='score', ascending=False).head(num_results)\n",
    "\n",
    "    results = []\n",
    "    for _, row in top_matches.iterrows():\n",
    "        img_url = row['medium_image_url']\n",
    "        brand = row['brand']\n",
    "        title = row['title']\n",
    "        price_raw = row['formatted_price']\n",
    "        score = row['score']\n",
    "\n",
    "        # Format price if possible\n",
    "        try:\n",
    "            price_val = int(price_raw)\n",
    "            price_str = f\"₹{price_val:,}\"\n",
    "        except:\n",
    "            price_str = \"N/A\"\n",
    "\n",
    "        # HTML layout\n",
    "        card_html = f\"\"\"\n",
    "        <div style=\"display:flex;align-items:center;border:1px solid #ccc;border-radius:10px;padding:10px;margin-bottom:10px;gap:20px;\">\n",
    "            <img src=\"{img_url}\" alt=\"image\" style=\"height:150px;border-radius:8px;transition:transform 0.3s ease-in-out;\" onmouseover=\"this.style.transform='scale(1.1)'\" onmouseout=\"this.style.transform='scale(1)'\">\n",
    "            <div>\n",
    "                <div><b>{title}</b></div>\n",
    "                <div>Brand: {brand}</div>\n",
    "                <div>Price: {price_str}</div>\n",
    "                <div>Similarity Score: {score:.2f}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        results.append(card_html)\n",
    "\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd377a90-77e9-487b-b689-450638cb79b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "gr.Interface(\n",
    "    fn=sbert_recommender_gradio,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter Product Title\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of Recommendations\")\n",
    "    ],\n",
    "    outputs=gr.HTML(),\n",
    "    title=\"Amazon Apparel Recommender (SBERT)\",\n",
    "    description=\"Enter a product title and get similar recommendations using SBERT.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3541de-f09a-4038-a077-b3024f83215e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f1931-ea0a-40b5-befd-65445d7c64f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2d68e-b14f-4b3b-a96a-85e0902ea5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cdb6e-7875-491e-bf30-c4ba8adc0ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c08ea2-4f6c-42ad-a357-1c4cae76e0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e68a0a-aed3-4850-9739-6a31e508b19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136be3a8-1904-4c22-94cb-2aff1020521f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dc657-b484-4829-810b-8b0a6a81ac4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a4bd7-b52c-486c-a3c7-471a4eb0a91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87413e-041f-48a9-89e1-5bf312d752bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e68323-af50-4be0-95dc-037a9ddb7bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb6349-4c25-4bd5-bdbd-088f7ba3c880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8eeb7b-45a1-451c-9f68-2e1271b06879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0cade-8c72-452e-88bd-3ccc604899fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
